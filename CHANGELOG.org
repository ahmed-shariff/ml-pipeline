* Change-log
** 2.0.a.6 [2020-04-12 Sun]
   - Moved the to mulitiprocessing from subprocess
   - Single mode now can test all versions of a single experiment
   - Implemetation of the iterator which breaks on test mode and Removed the `steps` from the main pipeline process
   - Added more flexibility to how datasets are loaded and moved Datasets to utils
     - train_dataset_file_path can now be empty
     - validation_data_load function as a new param
   - Moved the execute_experiment function to Process
   - Version loaded also shows progress in the form of [/]
   - Renamed dataset_file_path to data_asset in utils.Dataset
   - Log dataloader parameters if DataloaderWrapper class is used for the dataloader
   - Added API function to run experiment with instantiated experiment object
   - Method in Versions to add versions from another Versions object
** 2.0.a.4 [2019-07-24 Wed] 
   - Added function to load previously executed experiment.
   - The experiments are executed with the cwd set to the `experiments_dir`.
   - Added base implementation for pytroch.
   - Added a post_execution_hook, which operates the same as the pre_execution_hook and run after all train and evaluate loops are over.
   - Added current_version, experiment_dir and dataloader to be experiments properties
   - Moved to using multiprocessing from using subprocess
   - mlpipeline in single mode allows testing all versions of an experiment
   - Renamed cli script to '_cli'
   - Implemented iterator class which can be used to breaking during test mode
   - Since the above was one of the primary reason for the `steps` calculations, removed the `step` calculations from the main pipeline
   - Added more flexibility to how data is being loaded.
     - train_datasets_file_path can be empty
     - new parameter validation_data_load_function
     - If the input to the train or evaluation loop is None, the pipeline still attempts to execute the methods. If both train and test inputs are None, the evaluate_loop will be executed only once. If train input is None while test input is not None, evaluate_loop will execute twice: once with None as the input, and the second time with the test input.
   - None of the methods need to be implemented. If any method is not implemented, it'll simply log a message instead of throwing a tantrum.
   - Re-organized the modules and functions
   - Bumped minimum version of python to 3.5.5 to deal with the requirement of pandas
   - Bug fixes:
     - Moved each version of an experiment to be executed in a separate sub-process
     - Fixed bug with scheme and windows path for mlflow
     - Fixed bug with cli not completely migrated to using process abstraction
     - Ensured change to cwd when using api.get_experiment and exposed dataloader to the returned experiment
** 2.0.a.3 [2019-07-16]
   - Several methods from the base classes are not required to be implemented. They are:
     - Experiment.setup_model
     - Experiment.pre_execution_hook
     - Experiment.eval_loop
     - Dataloader.get_test_sample_count
     - Dataloader.get_train_sample_count
   - dataloader now can be None in a version.
   - Added DataLoadercallablewrapper class to base
   - Added is_no_log
   - Made mlflow as an install_dependency
   - mlflow now can be used in test mode as well. The tracking uri is set to the temp dir in test mode.
   - User now can set the mlflow tracking uri
   - Migrated cli interface to click and fixed bugs with SDK
** 2.0.a.2 [2019-07-10]
   - Major refactor: helper module renamed as base
   - Executed experiments in _pipeline_subprocess.py uses a class instead of dictionary
   - Added export mode
   - Added python API to execute mlpipeline
   - Added parameter agent to mlpipeline.utils.log
   - Fixed bugs as a result of new mlflow API
   - Moved loading scripts to separate function in mlpipeline.utils
** 1.1a3.post8 [2019-03-04]
   - Major refactor: renamed model to experiment.
   - Functions to track and copy files imported by the experiment scripts.
   - All files imported by an experiment script and the experiment script itself will be copied to the `experiment_dir` directory
   - Removed the use_history feature for now.
     - It seems to be buggy.
   - [[https://github.com/mlflow/mlflow][Mlflow]] integration.
     - If mlflow exists and use_mlflow is not set to `False`:
       - All mlflow related data will be stored in the outputs/mlrun directory
       - Each experiment script is considered a mlflow-experiment.
       - Each version executed form the experiment is considered a mlflow-run of the respective experiment.
       - The Matric.log_matrics will log all the metrics to mlflow also.
       - All the parameters passed through the version will be logged as parameters by mlflow.
