#+TODO: TODO(t) INPROGRESS(p) | DONE(d) CANCELED(c)
* Random thoughts
- How can the data pipeline be more disciplined?
  - Set up best practices
- Have one pipeline which is always running, and perhaps another script to send messages to the main pipeline in-place of  
- The pipeline is already implementation-agnostic, but how does that translate to a salable system? perhaps add another layer which is specific to the library on top of which a model is to b executed? 
- Online and offline pipelines? The current system is designed for offline training. 
- How can the deployment process be automated using the pipeline?
- Better version control for the models and data/metadata.
- Group a set of files as a experiment.
- Allow the model scripts to infer if the pipeline is in TEST or TRAIN mode.
- The classification steps passed to the training component, what if the steps calculated that way is too confusing? like I only want the number of epocs be passed?
- ~~ clean a easier logging.~~


* DONE Add a function that will be executed at the end of the loop, where I can add stuff like moving files, etc.
* TODO Commit following each train and eval loop.                       :1_9:
* DONE Is there a separate need for MODEL_DIR_SUFFIX? yes!              :1_9:
* TODO Rethink allow_delete_model_dir
* TODO Rethink how the training time is recorded to ensure a model that ended up failing to train can be relaunched. Or is this even an good behaviour to have?
* TODO Let the user override the checking-modified-time behaviour
* TODO mlflow integration [5/7]                                         :1_9:
** DONE Will be using the mlflow tracking interface
   CLOSED: [2019-03-03 Sun 20:12]
** DONE The experiment name will be the name of the script running [4/4]
*** DONE A script being launched by the subprocess is to be considered an experiment
    CLOSED: [2019-03-03 Sun 02:49]
*** DONE Each experiment can have a different set of versions, which will be represented by a run.
    CLOSED: [2019-03-03 Sun 02:49]
*** DONE Can an experiment have versions with the same name? Preferably not
    CLOSED: [2019-03-03 Sun 16:32]
    - Old versions will be mlflow_deleted. That is based on the assumption that the version is being overwritten.
*** DONE Are the mlrun stuff to be gitted? YES/for now
    CLOSED: [2019-03-03 Sun 02:52]
** DONE The metric_container will log all stuff that is logged when log_metrics is being called
   CLOSED: [2019-03-03 Sun 03:05]
** DONE The train and eval functions are expected to return a metric_container which will be logged in both normal and mlflow
   CLOSED: [2019-03-03 Sun 16:33]
   - The training output will not be logged by mlflow. Only the eval outputs will be logged, since that is what we want to look at. If someone want the train output they'd have to log it during the run.
** TODO The UI will be launched along side the pipeline. Also allow to launch the ui separately through the mlpipeline.
** DONE The files copied will also be logged through the mlflow artifacts
   CLOSED: [2019-03-03 Sun 21:10]
** TODO The version will log the values passed through it as parameters of the run
* TODO tensorboardx integration                                         :1_9:
* TODO Refactors [1/3]                                                 :1_10:
** TODO Rename model to experiment
** DONE Versions use easydict
   CLOSED: [2019-03-03 Sun 21:03]
** TODO Reduce the dependencies on Versions.
