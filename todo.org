* Random thoughts
- How can the data pipeline be more disciplined?
  - Set up best practices
- Have one pipeline which is always running, and perhaps another script to send messages to the main pipeline in-place of  
- The pipeline is already implementation-agnostic, but how does that translate to a salable system? perhaps add another layer which is specific to the library on top of which a model is to b executed? 
- Online and offline pipelines? The current system is designed for offline training. 
- How can the deployment process be automated using the pipeline?
- Better version control for the models and data/metadata.
- Commit following each train and eval loop.
- Group a set of files as a experiment.
- Add a function that will be executed at the end of the loop, where I can add stuff like moving files, etc.
- How can I handle situations where I want to relaunch the same experiment? use multiple versions with different names?
- The classification steps passed to the training component, what if the steps calculated that way is too confusing? like I only want the number of epocs be passed?
